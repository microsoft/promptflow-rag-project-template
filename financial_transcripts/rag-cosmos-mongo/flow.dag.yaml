id: template_chat_flow
name: Template Chat Flow
environment:
  python_requirements_txt: requirements.txt
inputs:
  chat_history:
    type: list
    default: []
    is_chat_input: false
    is_chat_history: true
  query:
    type: string
    default: " What is the growth rate of Azure ML revenue in FY23Q2?"
    is_chat_input: true
    is_chat_history: false
outputs:
  reply:
    type: string
    reference: ${GenerateResult.output}
    is_chat_output: true
  fetched_docs:
    type: string
    reference: ${FormatRetrievedDocuments.output}
  log_params:
    type: string
    reference: ${LogParam.output}
nodes:
- name: ConfigLoader
  type: python
  source:
    type: code
    path: ConfigLoader.py
  inputs:
    config_yaml: param_config.yaml
- name: CheckExistingContext
  type: python
  source:
    type: code
    path: CheckExistingContext.py
  inputs:
    history: ${inputs.chat_history}
- name: QueryParser
  type: python
  source:
    type: code
    path: QueryParser.py
  inputs:
    query: ${inputs.query}
    conversation: ${inputs.chat_history}
  aggregation: false
- name: ChatFlow
  type: python
  source:
    type: code
    path: ChatFlow.py
  inputs:
    check_existing_context: ${CheckExistingContext.output}
    filter: ${QueryParser.output}
    searchType: ${ConfigLoader.output.searchType}
- name: FormatRetrievedDocuments
  type: python
  source:
    type: code
    path: FormatRetrievedDocuments.py
  inputs:
    docs: ${Azure_CosmosDB_MongoDB_vCore_Vector_Search.output}
    maxTokens: ${ConfigLoader.output.maxTokens}
  use_variants: false
- name: FormatConversation
  type: python
  source:
    type: code
    path: FormatConversation.py
  inputs:
    history: ${inputs.chat_history}
    maxTokens: 800
  use_variants: false
- name: DetermineReply
  use_variants: true
- name: GetContextFromHistory
  type: python
  source:
    type: code
    path: GetContextFromHistory.py
  inputs:
    history: ${inputs.chat_history}
  activate:
    when: ${ChatFlow.output}
    is: use_same_context
- name: ContinueReply
  type: llm
  source:
    type: code
    path: ContinueReply.jinja2
  inputs:
    deployment_name: gpt-35-turbo
    conversation: ${FormatConversation.output}
    context: ${GetContextFromHistory.output}
    user_query: ${inputs.query}
  connection: aoai_connection
  api: chat
  activate:
    when: ${ChatFlow.output}
    is: use_same_context
- name: GenerateResult
  type: python
  source:
    type: code
    path: GenerateResult.py
  inputs:
    determine_reply: ${DetermineReply.output}
    continue_reply: ${ContinueReply.output}
    chat_flow: ${ChatFlow.output}
- name: LogParam
  type: python
  source:
    type: code
    path: LogParam.py
  inputs:
    filter: ${QueryParser.output}
    topK: ${ConfigLoader.output.topK}
    maxTokens: ${ConfigLoader.output.maxTokens}
    searchType: ${ConfigLoader.output.searchType}
    dbName: ${ConfigLoader.output.dbName)
- name: Embedding
  type: python
  source:
    type: package
    tool: promptflow.tools.embedding.embedding
  inputs:
    connection: aoai_connection
    input: ${inputs.query}
    deployment_name: text-embedding-ada-002
- name: Azure_CosmosDB_MongoDB_vCore_Vector_Search
  type: python
  source:
    type: package
    tool: pfazuredb.tools.mongodbvcore.vectorsearch
  inputs:
    connection: cosmodb_connection
    db_name: ${ConfigLoader.output.dbName}
    collection_name: ${ConfigLoader.output.colName}
    num_results: ${ConfigLoader.output.topK}
    embeddings: ${Embedding.output}
    filter_query: ${CollectionFilter.output}
    search_type: ${ConfigLoader.output.searchType}
- name: CollectionFilter
  type: python
  source:
    type: code
    path: CollectionFilter.py
  inputs:
    input1: ${QueryParser.output}
node_variants:
  DetermineReply:
    default_variant_id: variant_0
    variants:
      variant_0:
        node:
          type: llm
          source:
            type: code
            path: DetermineReply.jinja2
          inputs:
            deployment_name: gpt-35-turbo
            temperature: 0
            top_p: 1
            max_tokens: 800
            presence_penalty: 0
            frequency_penalty: 0
            conversation: ${FormatConversation.output}
            documentation: ${FormatRetrievedDocuments.output}
            user_query: ${inputs.query}
          connection: aoai_connection
          api: chat
          use_variants: false
          activate:
            when: ${ChatFlow.output}
            is: new_retrieval
      variant_1:
        node:
          type: llm
          source:
            type: code
            path: DetermineReply_variant_1.jinja2
          inputs:
            deployment_name: gpt-4
            temperature: 0
            top_p: 1
            max_tokens: 800
            presence_penalty: 0
            frequency_penalty: 0
            conversation: ${FormatConversation.output}
            documentation: ${FormatRetrievedDocuments.output}
            user_query: ${inputs.query}
          connection: aoai_connection
          api: chat
          use_variants: false
          activate:
            when: ${ChatFlow.output}
            is: new_retrieval

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmos DB PostgreSQL with pgvector\n",
    "\n",
    "This preprocessing notebook will guide you through chunking, embedding, and uploading to Cosmos DB PostgreSQL with the extension for pgvector. \n",
    "\n",
    "## Prerequisite\n",
    "- [Create a Cosmos DB for PostgreSQL cluster](https://learn.microsoft.com/en-us/azure/cosmos-db/postgresql/quickstart-create-portal?tabs=direct) in the Azure portal\n",
    "- Whitelist your IP to access you CosmosDB - PostgreSQL Cluster. Add you IP in \"Networking\" section of your PostgreSQL cluster\n",
    "- Add your connection string to the .`env` at the root of the repository. It should look something like \\\n",
    "    \"host={INSERT}.postgres.database.azure.com user={INSERT} dbname={INSERT} password={INSERT} sslmode=require\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you have already run the \"../../../preprocessing/step0_data_preprocessor.ipynb\" notebook to obtain DATA from the source (e.g. blobstorage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_loader = DirectoryLoader(\"../../../preprocessing/DATA\", glob=\"**/*.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docx_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_info_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Input: filename (\"MSFTTranscriptFY23Q4\")\n",
    "    Output: Extract stock symbol, year and quarter from filename\n",
    "    \"\"\"\n",
    "    pattern = r\"([A-Z]+)TranscriptFY(\\d{2})Q(\\d)\"\n",
    "    match = re.search(pattern, filename)\n",
    "\n",
    "    if match:\n",
    "        symbol = match.group(1)\n",
    "        fiscal_year = match.group(2)\n",
    "        fiscal_quarter = match.group(3)\n",
    "        return symbol, fiscal_year, fiscal_quarter\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = []\n",
    "\n",
    "for doc in docs:\n",
    "    source = doc.metadata[\"source\"]\n",
    "    symbol, fiscal_year, fiscal_quarter = extract_info_from_filename(source)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\n",
    "            \"\\n## \",\n",
    "            \"\\n### \",\n",
    "            \"\\n#### \",\n",
    "            \"\\n##### \",\n",
    "            \"\\n###### \",\n",
    "            \"```\\n\\n\",\n",
    "            \"\\n\\n***\\n\\n\",\n",
    "            \"\\n\\n---\\n\\n\",\n",
    "            \"\\n\\n___\\n\\n\",\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \"\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"source\": source,\n",
    "                \"symbol\": symbol,\n",
    "                \"fiscal_year\": fiscal_year,\n",
    "                \"fiscal_quarter\": fiscal_quarter,\n",
    "                \"chunk\": i,\n",
    "            },\n",
    "        )\n",
    "        doc_chunks.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# specify the name of the .env file name\n",
    "env_name = \"../../../../.env\"  # change to your own .env file name\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "if config[\"KEYS_FROM\"] == \"KEYVAULT\":\n",
    "    print(\"keyvault was selected.\")\n",
    "    keyVaultName = config[\"KEY_VAULT_NAME\"]\n",
    "    KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n",
    "\n",
    "    credential = DefaultAzureCredential()\n",
    "    client = SecretClient(vault_url=KVUri, credential=credential)\n",
    "\n",
    "    openai.api_type = client.get_secret(\"OPENAI-API-TYPE\").value\n",
    "    openai.api_key = client.get_secret(\"OPENAI-API-KEY\").value\n",
    "    openai.api_base = client.get_secret(\"OPENAI-API-BASE\").value\n",
    "    openai.api_version = client.get_secret(\"OPENAI-API-VERSION\").value\n",
    "    deployment_embedding = client.get_secret(\"OPENAI-DEPLOYMENT-EMBEDDING\").value\n",
    "else:\n",
    "    openai.api_type = config[\"OPENAI_API_TYPE\"]\n",
    "    openai.api_key = config[\"OPENAI_API_KEY\"]\n",
    "    openai.api_base = config[\"OPENAI_API_BASE\"]\n",
    "    openai.api_version = config[\"OPENAI_API_VERSION\"]\n",
    "    deployment_embedding = config[\"OPENAI_DEPLOYMENT_EMBEDDING\"]\n",
    "\n",
    "\n",
    "def createEmbeddings(text, endpoint, api_key, api_version, embedding_model_deployment):\n",
    "    request_url = f\"{endpoint}/openai/deployments/{embedding_model_deployment}/embeddings?api-version={api_version}\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": api_key}\n",
    "    request_payload = {\"input\": text}\n",
    "    embedding_response = requests.post(\n",
    "        request_url, json=request_payload, headers=headers, timeout=None\n",
    "    )\n",
    "    if embedding_response.status_code == 200:\n",
    "        data_values = embedding_response.json()[\"data\"]\n",
    "        embeddings_vectors = [data_value[\"embedding\"] for data_value in data_values]\n",
    "        return embeddings_vectors\n",
    "    else:\n",
    "        raise Exception(f\"failed to get embedding: {embedding_response.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "\n",
    "if config[\"KEYS_FROM\"] == \"KEYVAULT\":\n",
    "    print(\"keyvault was selected.\")\n",
    "    keyVaultName = config[\"KEY_VAULT_NAME\"]\n",
    "    KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n",
    "\n",
    "    credential = DefaultAzureCredential()\n",
    "    client = SecretClient(vault_url=KVUri, credential=credential)\n",
    "    POSTGRESQL_CONN_STRING = client.get_secret(\"COSMOS-DB-POSTGRESQL-CONN-STRING\").value\n",
    "else:\n",
    "    print(\".env was selected.\")\n",
    "    POSTGRESQL_CONN_STRING = config[\"COSMOS_DB_POSTGRESQL_CONN_STRING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, doc in enumerate(doc_chunks):\n",
    "    # Create embeddings using the provided function\n",
    "    embeddings = createEmbeddings(\n",
    "        doc.page_content,\n",
    "        openai.api_base,\n",
    "        openai.api_key,\n",
    "        openai.api_version,\n",
    "        deployment_embedding,\n",
    "    )[0]\n",
    "    data.append(\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"content\": doc.page_content,\n",
    "            \"embedding\": embeddings,\n",
    "            \"symbol\": doc.metadata[\"symbol\"],\n",
    "            \"fiscal_year\": doc.metadata[\"fiscal_year\"],\n",
    "            \"fiscal_quarter\": doc.metadata[\"fiscal_quarter\"],\n",
    "            \"source\": doc.metadata[\"source\"],\n",
    "            \"chunkid\": doc.metadata[\"chunk\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import psycopg2\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import ConnectionFailure\n",
    "from abc import ABC, abstractmethod\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from psycopg2 import pool\n",
    "from psycopg2 import Error\n",
    "from psycopg2 import sql\n",
    "\n",
    "\n",
    "class DatabaseService(ABC):\n",
    "    @abstractmethod\n",
    "    def store_data(self, data):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def retrieve_data(self, query, num_results):\n",
    "        pass\n",
    "\n",
    "\n",
    "class PostgresDBService(DatabaseService):\n",
    "    def __init__(self, table_name):\n",
    "        self.table_name = table_name\n",
    "        self._connect_db()\n",
    "        self._create_pg_extention()\n",
    "\n",
    "    def _connect_db(self):\n",
    "        postgreSQL_pool = pool.SimpleConnectionPool(1, 20, POSTGRESQL_CONN_STRING)\n",
    "        if postgreSQL_pool:\n",
    "            print(\"Connection pool created successfully\")\n",
    "        # Use getconn() to get a connection from the connection pool\n",
    "        self.connection = postgreSQL_pool.getconn()\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "    def _create_pg_extention(self):\n",
    "        try:\n",
    "            # Start a new transaction\n",
    "            self.cursor.execute(\"BEGIN;\")\n",
    "\n",
    "            # Previous transaction statements\n",
    "            # ...\n",
    "\n",
    "            # Check if the extension already exists\n",
    "            extension_query = \"SELECT * FROM pg_extension WHERE extname = 'vector';\"\n",
    "            self.cursor.execute(extension_query)\n",
    "            extension_exists = self.cursor.fetchone()\n",
    "\n",
    "            if not extension_exists:\n",
    "                # Extension does not exist, create it\n",
    "                create_extension_query = \"CREATE EXTENSION vector;\"\n",
    "                self.cursor.execute(create_extension_query)\n",
    "                self.connection.commit()\n",
    "            else:\n",
    "                # Extension already exists, pass through\n",
    "                pass\n",
    "\n",
    "            # Commit the transaction\n",
    "            self.cursor.execute(\"COMMIT;\")\n",
    "        except Exception as e:\n",
    "            # An error occurred, rollback the transaction\n",
    "            self.cursor.execute(\"ROLLBACK;\")\n",
    "            raise e\n",
    "        finally:\n",
    "            # Close the cursor\n",
    "            self.cursor.close()\n",
    "\n",
    "    def check_pgvector_connection(self):\n",
    "        try:\n",
    "            self.cursor = self.connection.cursor()\n",
    "            # Define the SHOW EXTENSIONS query\n",
    "            show_extensions_query = \"SHOW azure.extensions;\"\n",
    "\n",
    "            # Execute the SHOW EXTENSIONS query\n",
    "            self.cursor.execute(show_extensions_query)\n",
    "\n",
    "            self.connection.commit()\n",
    "            # Fetch and print the results\n",
    "            results = self.cursor.fetchall()\n",
    "            for row in results:\n",
    "                print(row)\n",
    "            self.cursor.close()\n",
    "        except:\n",
    "            print(\"Warning: could not check the pg vector extension\")\n",
    "        finally:\n",
    "            self.cursor.close()\n",
    "\n",
    "    def create_schema(self, doc_chunks):\n",
    "        import pandas as pd\n",
    "\n",
    "        data = []\n",
    "        for i, doc in enumerate(doc_chunks):\n",
    "            # Create embeddings using the provided function\n",
    "            embeddings = createEmbeddings(\n",
    "                doc.page_content,\n",
    "                openai.api_base,\n",
    "                openai.api_key,\n",
    "                openai.api_version,\n",
    "                deployment_embedding,\n",
    "            )[0]\n",
    "            data.append(\n",
    "                {\n",
    "                    \"id\": i,\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"embedding\": embeddings,\n",
    "                    \"symbol\": doc.metadata[\"symbol\"],\n",
    "                    \"fiscal_year\": doc.metadata[\"fiscal_year\"],\n",
    "                    \"fiscal_quarter\": doc.metadata[\"fiscal_quarter\"],\n",
    "                    \"source\": doc.metadata[\"source\"],\n",
    "                    \"chunkid\": doc.metadata[\"chunk\"],\n",
    "                }\n",
    "            )\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def store_data(self, df):\n",
    "        # Convert the DataFrame to a list of tuples for bulk insertion\n",
    "        records = df.to_records(index=False)\n",
    "        records_list = records.tolist()\n",
    "\n",
    "        # Open a cursor to perform database operations\n",
    "        cursor = self.connection.cursor()\n",
    "\n",
    "        # Define the table name\n",
    "        table_name = self.table_name\n",
    "        batch_size = 10\n",
    "\n",
    "        # Execute the query to check if the table exists\n",
    "        cursor.execute(\n",
    "            f\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = '{table_name}');\"\n",
    "        )\n",
    "\n",
    "        # Fetch the result\n",
    "        exists = cursor.fetchone()[0]\n",
    "\n",
    "        if exists:\n",
    "            print(f\"The table '{table_name}' exists in the database.\")\n",
    "            print(\n",
    "                \"You may drop previous table (see commented code above) if you want to re-insert reviews.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"The table '{table_name}' does not exist in the database. Creating it now and inserting data ...\"\n",
    "            )\n",
    "\n",
    "            # Use getconn() to get a connection from the connection pool\n",
    "            with self.connection as connection:\n",
    "                # Define the CREATE TABLE query\n",
    "                create_table_query = f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                    Id INTEGER PRIMARY KEY,\n",
    "                    Content TEXT,\n",
    "                    Embedding VECTOR,\n",
    "                    Symbol TEXT,\n",
    "                    FiscalYear TEXT,\n",
    "                    FiscalQuarter INTEGER,\n",
    "                    Source TEXT,\n",
    "                    ChunkId TEXT\n",
    "                );\n",
    "                \"\"\"\n",
    "                # Execute the CREATE TABLE query\n",
    "                cursor.execute(create_table_query)\n",
    "                connection.commit()\n",
    "\n",
    "                # Define the INSERT INTO query\n",
    "                insert_query = (\n",
    "                    f\"INSERT INTO {table_name} (Id, Content, Embedding, Symbol, FiscalYear, FiscalQuarter, Source, ChunkId) \"\n",
    "                    f\"VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "                )\n",
    "\n",
    "                # Execute the INSERT INTO query for each row\n",
    "                cursor.executemany(insert_query, records_list)\n",
    "                connection.commit()\n",
    "\n",
    "                # Execute the CREATE TABLE query\n",
    "                try:\n",
    "                    with connection.cursor() as cursor:\n",
    "                        cursor.execute(create_table_query)\n",
    "                        connection.commit()\n",
    "                        print(f\"Table {table_name} created successfully!\")\n",
    "                except (Exception, Error) as e:\n",
    "                    print(f\"Error creating table {table_name}: {e}\")\n",
    "                    connection.rollback()\n",
    "\n",
    "                # Convert numpy.int32 to int in each row\n",
    "                records_list = [\n",
    "                    tuple(\n",
    "                        int(value) if isinstance(value, np.int32) else value\n",
    "                        for value in record\n",
    "                    )\n",
    "                    for record in records_list\n",
    "                ]\n",
    "\n",
    "                # Split the records list into batches\n",
    "                batches = [\n",
    "                    records_list[i : i + batch_size]\n",
    "                    for i in range(0, len(records_list), batch_size)\n",
    "                ]\n",
    "\n",
    "                # Iterate over each batch and perform bulk insert\n",
    "                count = 0\n",
    "                for batch in batches:\n",
    "                    count += 1\n",
    "                    print(f\"Inserting batch {count} into the table\")\n",
    "                    try:\n",
    "                        insert_query = sql.SQL(\n",
    "                            f\"INSERT INTO {table_name} (Id, Content, Embedding, Symbol, FiscalYear, FiscalQuarter, Source, ChunkId) \"\n",
    "                            f\"VALUES ({', '.join(['%s'] * len(batch[0]))})\"\n",
    "                        )\n",
    "\n",
    "                        with connection.cursor() as cursor:\n",
    "                            cursor.executemany(insert_query, batch)\n",
    "                            connection.commit()\n",
    "                    except (Exception, Error) as e:\n",
    "                        print(f\"Error inserting batch into the table: {e}\")\n",
    "                        connection.rollback()\n",
    "\n",
    "    def retrieve_data(self, query, num_results=3):\n",
    "        # Register 'pgvector' type for the 'embedding' column\n",
    "        register_vector(self.connection)\n",
    "        queryEmbedding = createEmbeddings(\n",
    "            query,\n",
    "            openai.api_base,\n",
    "            openai.api_key,\n",
    "            openai.api_version,\n",
    "            deployment_embedding,\n",
    "        )[0]\n",
    "\n",
    "        select_query = f\"SELECT id FROM {self.table_name} ORDER BY embedding <-> %s LIMIT {num_results}\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(select_query, (np.array(queryEmbedding),))\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Use the top k ids to retrieve the actual text from the database\n",
    "        top_ids = []\n",
    "        for i in range(len(results)):\n",
    "            top_ids.append(int(results[i][0]))\n",
    "\n",
    "        self.connection.rollback()\n",
    "\n",
    "        format_ids = \", \".join([\"%s\"] * len(top_ids))\n",
    "\n",
    "        sql = f\"SELECT CONCAT('Content: ', Content, 'Symbol:', Symbol, ' ', 'FiscalYear: ', FiscalYear, ' ', 'FiscalQuarter: ', FiscalQuarter, ' ', 'Source: ', Source) AS concat FROM {self.table_name} WHERE id IN ({format_ids})\"\n",
    "\n",
    "        # Execute the SELECT statement\n",
    "        try:\n",
    "            cursor.execute(sql, top_ids)\n",
    "            top_rows = cursor.fetchall()\n",
    "            output = []\n",
    "            for row in top_rows:\n",
    "                output.append(row)\n",
    "        except (Exception, Error) as e:\n",
    "            print(f\"Error executing SELECT statement: {e}\")\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdb = PostgresDBService(table_name=\"msft_transcript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdb.check_pgvector_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = testdb.create_schema(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdb.store_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = testdb.retrieve_data(\"what is the growth rate for azure ml revenue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appliedaipf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

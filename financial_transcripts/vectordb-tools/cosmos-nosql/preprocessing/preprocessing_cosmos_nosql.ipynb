{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmos DB NoSQL\n",
    "\n",
    "This preprocessing notebook will guide you through chunking, embedding, and uploading to Cosmos DB NOSQL.\n",
    "\n",
    "## Prerequisite\n",
    "- [Create a Cosmos DB NoSQL database according to the microsoft documentation](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/) in the Azure portal\n",
    "- Add your connection string to the .`env` at the root of the repository. It should look something like \\\n",
    "    \"AccountEndpoint=https://{INSERT}.documents.azure.com:443/;AccountKey={INSERT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you have already run the \"../../../preprocessing/step0_data_preprocessor.ipynb\" notebook to obtain DATA from the source (e.g. blobstorage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_loader = DirectoryLoader(\"../../../preprocessing/DATA\", glob=\"**/*.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docx_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_info_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Input: filename (\"MSFTTranscriptFY23Q4\")\n",
    "    Output: Extract stock symbol, year and quarter from filename\n",
    "    \"\"\"\n",
    "    pattern = r\"([A-Z]+)TranscriptFY(\\d{2})Q(\\d)\"\n",
    "    match = re.search(pattern, filename)\n",
    "\n",
    "    if match:\n",
    "        symbol = match.group(1)\n",
    "        fiscal_year = match.group(2)\n",
    "        fiscal_quarter = match.group(3)\n",
    "        return symbol, fiscal_year, fiscal_quarter\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = []\n",
    "\n",
    "for doc in docs:\n",
    "    source = doc.metadata[\"source\"]\n",
    "    symbol, fiscal_year, fiscal_quarter = extract_info_from_filename(source)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\n",
    "            \"\\n## \",\n",
    "            \"\\n### \",\n",
    "            \"\\n#### \",\n",
    "            \"\\n##### \",\n",
    "            \"\\n###### \",\n",
    "            \"```\\n\\n\",\n",
    "            \"\\n\\n***\\n\\n\",\n",
    "            \"\\n\\n---\\n\\n\",\n",
    "            \"\\n\\n___\\n\\n\",\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \"\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"source\": source,\n",
    "                \"symbol\": symbol,\n",
    "                \"fiscal_year\": fiscal_year,\n",
    "                \"fiscal_quarter\": fiscal_quarter,\n",
    "                \"chunk\": i,\n",
    "            },\n",
    "        )\n",
    "        doc_chunks.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# specify the name of the .env file name\n",
    "env_name = \"../../../../.env\"  # change to your own .env file name\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "if config[\"KEYS_FROM\"] == \"KEYVAULT\":\n",
    "    print(\"keyvault was selected.\")\n",
    "    keyVaultName = config[\"KEY_VAULT_NAME\"]\n",
    "    KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n",
    "\n",
    "    credential = DefaultAzureCredential()\n",
    "    client = SecretClient(vault_url=KVUri, credential=credential)\n",
    "\n",
    "    openai.api_type = client.get_secret(\"OPENAI-API-TYPE\").value\n",
    "    openai.api_key = client.get_secret(\"OPENAI-API-KEY\").value\n",
    "    openai.api_base = client.get_secret(\"OPENAI-API-BASE\").value\n",
    "    openai.api_version = client.get_secret(\"OPENAI-API-VERSION\").value\n",
    "    deployment_embedding = client.get_secret(\"OPENAI-DEPLOYMENT-EMBEDDING\").value\n",
    "else:\n",
    "    openai.api_type = config[\"OPENAI_API_TYPE\"]\n",
    "    openai.api_key = config[\"OPENAI_API_KEY\"]\n",
    "    openai.api_base = config[\"OPENAI_API_BASE\"]\n",
    "    openai.api_version = config[\"OPENAI_API_VERSION\"]\n",
    "    deployment_embedding = config[\"OPENAI_DEPLOYMENT_EMBEDDING\"]\n",
    "\n",
    "\n",
    "def createEmbeddings(text, endpoint, api_key, api_version, embedding_model_deployment):\n",
    "    request_url = f\"{endpoint}/openai/deployments/{embedding_model_deployment}/embeddings?api-version={api_version}\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": api_key}\n",
    "    request_payload = {\"input\": text}\n",
    "    embedding_response = requests.post(\n",
    "        request_url, json=request_payload, headers=headers, timeout=None\n",
    "    )\n",
    "    if embedding_response.status_code == 200:\n",
    "        data_values = embedding_response.json()[\"data\"]\n",
    "        embeddings_vectors = [data_value[\"embedding\"] for data_value in data_values]\n",
    "        return embeddings_vectors\n",
    "    else:\n",
    "        raise Exception(f\"failed to get embedding: {embedding_response.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "\n",
    "if config[\"KEYS_FROM\"] == \"KEYVAULT\":\n",
    "    print(\"keyvault was selected.\")\n",
    "    keyVaultName = config[\"KEY_VAULT_NAME\"]\n",
    "    KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n",
    "\n",
    "    credential = DefaultAzureCredential()\n",
    "    client = SecretClient(vault_url=KVUri, credential=credential)\n",
    "    NOSQL_CONN_STRING = client.get_secret(\"COSMOS-DB-NOSQL-CONN-STRING\").value\n",
    "else:\n",
    "    print(\".env was selected.\")\n",
    "    NOSQL_CONN_STRING = config[\"COSMOS_DB_NOSQL_CONN_STRING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, doc in enumerate(doc_chunks):\n",
    "    # Create embeddings using the provided function\n",
    "    content_embeddings = createEmbeddings(\n",
    "        doc.page_content,\n",
    "        openai.api_base,\n",
    "        openai.api_key,\n",
    "        openai.api_version,\n",
    "        deployment_embedding,\n",
    "    )[0]\n",
    "    source_embeddings = createEmbeddings(\n",
    "        doc.metadata[\"source\"],\n",
    "        openai.api_base,\n",
    "        openai.api_key,\n",
    "        openai.api_version,\n",
    "        deployment_embedding,\n",
    "    )[0]\n",
    "    data.append(\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"content\": doc.page_content,\n",
    "            \"content_vector\": content_embeddings,\n",
    "            \"source_vector\": source_embeddings,\n",
    "            \"symbol\": doc.metadata[\"symbol\"],\n",
    "            \"fiscal_year\": doc.metadata[\"fiscal_year\"],\n",
    "            \"fiscal_quarter\": doc.metadata[\"fiscal_quarter\"],\n",
    "            \"source\": doc.metadata[\"source\"],\n",
    "            \"chunkid\": doc.metadata[\"chunk\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [\n",
    "        {\n",
    "            \"path\": \"/contentVector\",\n",
    "            \"dataType\": \"float32\",\n",
    "            \"distanceFunction\": \"dotproduct\",\n",
    "            \"dimensions\": 1536,\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/sourceVector\",\n",
    "            \"dataType\": \"float32\",\n",
    "            \"distanceFunction\": \"cosine\",\n",
    "            \"dimensions\": 1536,\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: in the cell below we exclude '_etag' so any changes to it will not invoke re-indexing. We also exclude the source vector and content vector as their change is usually accomodateed by change in source and content, which will invoke re-indexing regardless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_policy = {\n",
    "    \"includedPaths\": [{\"path\": \"/*\"}],\n",
    "    \"excludedPaths\": [\n",
    "        {\"path\": '/\"_etag\"/?'},\n",
    "        {\"path\": \"/source_vector/*\"},\n",
    "        {\"path\": \"/content_vector/*\"},\n",
    "    ],\n",
    "    \"vectorIndexes\": [\n",
    "        {\"path\": \"/sourceVector\", \"type\": \"quantizedFlat\"},\n",
    "        {\"path\": \"/contentVector\", \"type\": \"quantizedFlat\"},\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_INDEX_CONFIG = {\n",
    "    \"indexingPolicy\": indexing_policy,\n",
    "    \"vectorEmbeddingPolicy\": vector_embedding_policy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import CosmosClient\n",
    "\n",
    "COSMOS_NOSQL_CLIENT = CosmosClient.from_connection_string(NOSQL_CONN_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosmos DB imports\n",
    "from azure.cosmos import CosmosClient\n",
    "from azure.cosmos.aio import CosmosClient as CosmosAsyncClient\n",
    "from azure.cosmos import PartitionKey, exceptions\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DatabaseService(ABC):\n",
    "    @abstractmethod\n",
    "    def store_data(self, data):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def retrieve_data(self, query, num_results):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NOSQLDBService(DatabaseService):\n",
    "    def __init__(\n",
    "        self, db_name, container_name, search_index_config=SEARCH_INDEX_CONFIG\n",
    "    ):\n",
    "        self.db_name = db_name\n",
    "        self.container_name = container_name\n",
    "        self.search_index_config = search_index_config\n",
    "        self.client = COSMOS_NOSQL_CLIENT\n",
    "        self._create_db()\n",
    "        self._create_container()\n",
    "\n",
    "    def _create_db(self):\n",
    "        import json\n",
    "\n",
    "        self.db = self.client.create_database_if_not_exists(id=self.db_name)\n",
    "        self.db_properties = self.db.read()\n",
    "        print(json.dumps(self.db_properties))\n",
    "\n",
    "    def _create_container(self):\n",
    "        try:\n",
    "            self.container = self.db.create_container_if_not_exists(\n",
    "                id=self.container_name,\n",
    "                partition_key=PartitionKey(path=\"/id\", kind=\"Hash\"),\n",
    "                indexing_policy=self.search_index_config[\"indexingPolicy\"],\n",
    "                vector_embedding_policy=self.search_index_config[\n",
    "                    \"vectorEmbeddingPolicy\"\n",
    "                ],\n",
    "            )\n",
    "        except exceptions.CosmosResourceExistsError:\n",
    "            print(f\"Container {self.container_name} already exists.\")\n",
    "            self.container = self.db.get_container_client(self.container_name)\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Failed to create container {self.container_name}: {e}\")\n",
    "\n",
    "    def create_schema(self, doc_chunks):\n",
    "        data = []\n",
    "        for i, doc in enumerate(doc_chunks):\n",
    "            # Create embeddings using the provided function\n",
    "            content_embeddings = createEmbeddings(\n",
    "                doc.page_content,\n",
    "                openai.api_base,\n",
    "                openai.api_key,\n",
    "                openai.api_version,\n",
    "                deployment_embedding,\n",
    "            )[0]\n",
    "            source_embeddings = createEmbeddings(\n",
    "                doc.metadata[\"source\"],\n",
    "                openai.api_base,\n",
    "                openai.api_key,\n",
    "                openai.api_version,\n",
    "                deployment_embedding,\n",
    "            )[0]\n",
    "            data.append(\n",
    "                {\n",
    "                    \"id\": str(i),\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"contentVector\": content_embeddings,\n",
    "                    \"sourceVector\": source_embeddings,\n",
    "                    \"symbol\": doc.metadata[\"symbol\"],\n",
    "                    \"fiscal_year\": doc.metadata[\"fiscal_year\"],\n",
    "                    \"fiscal_quarter\": doc.metadata[\"fiscal_quarter\"],\n",
    "                    \"source\": doc.metadata[\"source\"],\n",
    "                    \"chunkid\": doc.metadata[\"chunk\"],\n",
    "                    \"@search.action\": \"upload\",\n",
    "                }\n",
    "            )\n",
    "        return data\n",
    "\n",
    "    def store_data(self, data):\n",
    "        # Convert the DataFrame to a list of tuples for bulk insertion\n",
    "        for item in data:\n",
    "            print(\"writing item \", item[\"id\"])\n",
    "            try:\n",
    "                self.container.upsert_item(item)\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting item: {e}\")\n",
    "\n",
    "    def retrieve_data(self, query, num_results=3):\n",
    "        # Register 'pgvector' type for the 'embedding' column\n",
    "        queryEmbedding = createEmbeddings(\n",
    "            query,\n",
    "            openai.api_base,\n",
    "            openai.api_key,\n",
    "            openai.api_version,\n",
    "            deployment_embedding,\n",
    "        )[0]\n",
    "        output = self.container.query_items(\n",
    "            query=\"SELECT TOP @num_results c.content, c.symbol, c.fiscal_year,c.fiscal_quarter, VectorDistance(c.content_vector,@embedding) AS SimilarityScore  FROM c ORDER BY VectorDistance(c.content_vector,@embedding)\",\n",
    "            parameters=[\n",
    "                {\"name\": \"@embedding\", \"value\": queryEmbedding},\n",
    "                {\"name\": \"@num_results\", \"value\": num_results},\n",
    "            ],\n",
    "            enable_cross_partition_query=True,\n",
    "        )\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdb = NOSQLDBService(db_name=\"promptflow_sample\", container_name=\"ms_transcripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = testdb.create_schema(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdb.store_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = testdb.retrieve_data(\"what is the growth rate for azure ml revenue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    print(f\"result {i}:\\n\\n\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(f\"Symbol: {result['symbol']}\")\n",
    "    print(f\"Fiscal Year: {result['fiscal_year']}\")\n",
    "    print(f\"Fiscal Quarter: {result['fiscal_quarter']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appliedaipf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
